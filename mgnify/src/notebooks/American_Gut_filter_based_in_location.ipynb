{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Gut Project example\n",
    "\n",
    "This notebook was created from a question we recieved from a user of MGnify.\n",
    "\n",
    "The question was:\n",
    "\n",
    "```\n",
    "I am attempting to retrieve some of the MGnify results from samples that are part of the American Gut Project based on sample location. \n",
    "However latitude and longitude do not appear to be searchable fields. \n",
    "Is it possible to query these fields myself or to work with someone to retrieve a list of samples from a specific geographic range? I am interested in samples from people in Hawaii, so 20.5 - 20.7 and -154.0 - -161.2.\n",
    "```\n",
    "\n",
    "Let's decompose the question:\n",
    "- project \"American Gut Project\"\n",
    "- Metadata filtration using the geographic location of a sample. \n",
    "-   Get samples for Hawai: 20.5 - 20.7 ; -154.0 - -161.2\n",
    "\n",
    "Each sample if MGnify it's obtained from [ENA](https://www.ebi.ac.uk/ena).\n",
    "\n",
    "## Get samples\n",
    "\n",
    "The first step is to obtain the samples using [ENA advanced search API](https://www.ebi.ac.uk/ena/browser/advanced-search).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "secondary_sample_accession   lat     lon\naccession                                              \nSAMEA104163502                 ERS1822520  19.6  -155.0\nSAMEA104163503                 ERS1822521  19.6  -155.0\nSAMEA104163504                 ERS1822522  19.6  -155.0\nSAMEA104163505                 ERS1822523  19.6  -155.0\nSAMEA104163506                 ERS1822524  19.6  -155.0\n...                                   ...   ...     ...\nSAMEA4588733                   ERS2409455  21.5  -157.8\nSAMEA4588734                   ERS2409456  21.5  -157.8\nSAMEA4786501                   ERS2606437  21.4  -157.7\nSAMEA92368918                  ERS1561273  19.4  -155.0\nSAMEA92936668                  ERS1562030  21.3  -157.7\n\n[121 rows x 3 columns]\n"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "import requests\n",
    "\n",
    "base_url = 'https://www.ebi.ac.uk/ena/portal/api/search' \n",
    "\n",
    "# parameters\n",
    "params = {\n",
    "    'result': 'sample',\n",
    "    'query': ' AND '.join([\n",
    "        'geo_box1(16.9175,-158.4687,21.6593,-152.7969)',\n",
    "        'description=\"*American Gut Project*\"'\n",
    "    ]),\n",
    "    'fields': ','.join(['secondary_sample_accession', 'lat', 'lon']),\n",
    "    'format': 'json',\n",
    "}\n",
    "\n",
    "response = requests.post(base_url, data=params)\n",
    "\n",
    "agp_samples = response.json()\n",
    "\n",
    "df = DataFrame(columns=('secondary_sample_accession', 'lat', 'lon'))\n",
    "df.index.name = 'accession'\n",
    "\n",
    "for s in agp_samples:\n",
    "    df.loc[s.get('accession')] = [\n",
    "        s.get('secondary_sample_accession'),\n",
    "        s.get('lat'),\n",
    "        s.get('lon')\n",
    "    ]\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use EMG API to get the information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/usr/env python\n",
    "\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_links(data):\n",
    "    return data[\"links\"][\"related\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    samples_url = \"https://www.ebi.ac.uk/metagenomics/api/v1/samples/\"\n",
    "   \n",
    "    tsv = sys.argv[1] if len(sys.argv) == 2 else None\n",
    "    if not tsv:\n",
    "        print(\"The first arg is the tsv file\")\n",
    "        exit(1)\n",
    "\n",
    "    tsv_fh = open(tsv, \"r\")\n",
    "\n",
    "    # header\n",
    "    next(tsv_fh)\n",
    "\n",
    "    for record in tsv_fh:\n",
    "        # get the runs first\n",
    "\n",
    "        # mgnify references the secondary accession\n",
    "        _, sec_acc, *_ = record.split(\"\\t\")\n",
    "        samples_res = requests.get(samples_url + sec_acc)\n",
    "\n",
    "        if samples_res.status_code == 404:\n",
    "            print(sec_acc + \" not found in MGnify\")\n",
    "            continue\n",
    "\n",
    "        # then the analysis for that run\n",
    "        runs_url = get_links(samples_res.json()[\"data\"][\"relationships\"][\"runs\"])\n",
    "\n",
    "        if not runs_url:\n",
    "            print(\"No runs for sample \" + sec_acc)\n",
    "            continue\n",
    "\n",
    "        print(\"Getting the runs: \" + runs_url)\n",
    "\n",
    "        run_res = requests.get(runs_url)\n",
    "\n",
    "        if run_res.status_code != 200:\n",
    "            print(run_url + \" failed\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        # iterate over the sample runs\n",
    "        run_data = run_res.json()\n",
    "\n",
    "        # this script doesn't consider pagination, it's just an example\n",
    "        # there could be more that one page of runs\n",
    "        # use links -> next to get the next page\n",
    "        for run in run_data[\"data\"]:\n",
    "            analyses_url = get_links(run[\"relationships\"][\"analyses\"])\n",
    "\n",
    "            if not analyses_url:\n",
    "                print(\"No analyses for run \" + run)\n",
    "                continue\n",
    "\n",
    "            analyses_res = requests.get(analyses_url)\n",
    "\n",
    "            if analyses_res.status_code != 200:\n",
    "               print(analyses_url + \" failed\", file=sys.stderr)\n",
    "               continue\n",
    "\n",
    "            # dump\n",
    "            print(\"Raw analyses data\")\n",
    "            print(analyses_res.json())\n",
    "            print(\"=\" * 30)\n",
    "\n",
    "    tsv_fh.close()"
   ]
  }
 ]
}